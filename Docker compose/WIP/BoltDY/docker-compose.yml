services:
  bolt-diy:
    build: https://github.com/stackblitz-labs/bolt.diy
    image: bolt-ai:development
    build:
      target: bolt-ai-development
    env_file: '.env.local'
    environment:
      - NODE_ENV=development
      - VITE_HMR_PROTOCOL=ws
      - VITE_HMR_HOST=localhost
      - VITE_HMR_PORT=5173
      - CHOKIDAR_USEPOLLING=true
      - WATCHPACK_POLLING=true
      - PORT=5173
      - GROQ_API_KEY=${GROQ_API_KEY}
      - HuggingFace_API_KEY=${HUGGING_FACE_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPEN_ROUTER_API_KEY=${OPEN_ROUTER_API_KEY}
      - XAI_API_KEY=${XAI_API_KEY}
      - GOOGLE_GENERATIVE_AI_API_KEY=${GEMINI_API_KEY}
      - OLLAMA_API_BASE_URL=${http://127.0.0.1:11434}
      - TOGETHER_API_KEY=${TOGETHER_API_KEY}
      - TOGETHER_API_BASE_URL=${TOGETHER_API_BASE_URL}
      - AWS_BEDROCK_CONFIG=${AWS_BEDROCK_CONFIG}
      - VITE_LOG_LEVEL=${VITE_LOG_LEVEL:-debug}
      - DEFAULT_NUM_CTX=${DEFAULT_NUM_CTX:-32768}
      - RUNNING_IN_DOCKER=true
    extra_hosts:
      - 'host.docker.internal:host-gateway'
    volumes:
      - type: bind
        source: .
        target: /app
        consistency: cached
      - /app/node_modules
    ports:
      - '5173:5173'
    command: pnpm run dev --host 0.0.0.0
    profiles: ['development', 'default']
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-local:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434  # Ensures binding to all network interfaces
      - OLLAMA_DEBUG=true
    restart: unless-stopped
  OpenWebUI:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - "8080:8080"  # Maps OpenWebUI default port to host
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - open-webui-local:/app/backend/data